{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST with PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5g7Pmbu5aOVGiBu7mfpRO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5TYIMmzCaui"
      },
      "source": [
        "<h1><b>MNIST with PyTorch</b></h1>\n",
        "\n",
        "Start with common methods and classes that will be used in the subsequent experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8vHeHGxd3TV"
      },
      "source": [
        "<h2>Generic classes and functions</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pidMZaALOl-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10d548b-0ff4-4983-feab-81f4386cec18"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision\n",
        "from collections import OrderedDict\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, sequence):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.nn_stack = nn.Sequential(sequence)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        return self.nn_stack(x)\n",
        "\n",
        "# Stolen from Shweta's code\n",
        "def one_hot(y):\n",
        "    one_hot = torch.zeros([1, 10])\n",
        "    one_hot[0][y] = 1\n",
        "    return one_hot\n",
        "\n",
        "def load_data(one_hot_transform = False, batch_size = 10):\n",
        "    transformation = None\n",
        "    if one_hot_transform:\n",
        "        transformation = Lambda(lambda y: one_hot(y))\n",
        "    \n",
        "    training_data = datasets.MNIST(\n",
        "        root=\"data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=ToTensor(),\n",
        "        target_transform=transformation\n",
        "    )\n",
        "\n",
        "    test_data = datasets.MNIST(\n",
        "        root=\"data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=ToTensor(),\n",
        "        target_transform=transformation\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
        "    test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
        "\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using\", device)"
      ],
      "execution_count": 446,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp2Ur14o9sBn"
      },
      "source": [
        "<h2>1 - Mean squared error loss function, sigmoid activation and 1 hidden layer<h2>\n",
        "\n",
        "<strong>Network Summary</strong>\n",
        "\n",
        "*   Input layer: 784 nodes\n",
        "*   Hidden layer: 30 nodes\n",
        "*   Output layer: 10 nodes\n",
        "*   Activation between nodes: sigmoid\n",
        "*   Output activation: sigmoid\n",
        "*   Loss function: mean squared error\n",
        "*   Stochastic gradient descent 10 mini-batches\n",
        "*   5 epochs, learning rate = 0.3\n",
        "\n",
        "<strong>Results</strong>\n",
        "\n",
        "*   Training set: 95.3%\n",
        "*   Testing set: 94.7%\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nLOd_-SqEHi"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    correct = 0\n",
        "    size = len(dataloader.dataset)\n",
        "    for X, y in dataloader.dataset:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        correct += 1 if (pred.argmax(1) == y.argmax(1)) else 0\n",
        "    correct /= size\n",
        "    print(f\"Training Accuracy: {(100*correct):>0.1f}%\")"
      ],
      "execution_count": 440,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l8i75Wtq-hf"
      },
      "source": [
        "def test(dataloader, model):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader.dataset:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            correct += 1 if (pred.argmax(1) == y.argmax(1)) else 0\n",
        "    correct /= size\n",
        "    print(f\"Test Accuracy: {(100*correct):>0.1f}%\")"
      ],
      "execution_count": 447,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAFiSZ74rSwG"
      },
      "source": [
        "sequence = OrderedDict([\n",
        "            ('linear1', nn.Linear(784, 30)),\n",
        "            ('sigmoid1', nn.Sigmoid()),\n",
        "            ('linear2', nn.Linear(30, 10)),\n",
        "            ('sigmoid2', nn.Sigmoid())\n",
        "])\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = True, batch_size = 10)\n",
        "model = NeuralNetwork(sequence).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.MSELoss()\n"
      ],
      "execution_count": 448,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyIS78F2uIlx",
        "outputId": "36567a6b-f571-4f45-e637-4ca1c9e24737"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    print(\"-------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)"
      ],
      "execution_count": 449,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------\n",
            "Training Accuracy: 86.9%\n",
            "Test Accuracy: 92.2%\n",
            "\n",
            "Epoch 2\n",
            "-------\n",
            "Training Accuracy: 93.3%\n",
            "Test Accuracy: 93.6%\n",
            "\n",
            "Epoch 3\n",
            "-------\n",
            "Training Accuracy: 94.3%\n",
            "Test Accuracy: 94.2%\n",
            "\n",
            "Epoch 4\n",
            "-------\n",
            "Training Accuracy: 94.9%\n",
            "Test Accuracy: 94.5%\n",
            "\n",
            "Epoch 5\n",
            "-------\n",
            "Training Accuracy: 95.3%\n",
            "Test Accuracy: 94.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5jA9Ox4Dv4m"
      },
      "source": [
        "<h2>2 - Using cross entrophy loss function</h2>\n",
        "\n",
        "*   Input layer: 784 nodes\n",
        "*   Hidden layer: 30 nodes\n",
        "*   Output layer: 10 nodes\n",
        "*   Activation between nodes: sigmoid\n",
        "*   Output activation: sigmoid\n",
        "*   Loss function: cross-entropy\n",
        "*   Stochastic gradient descent 10 mini-batches\n",
        "*   5 epochs, learning rate = 0.3\n",
        "\n",
        "<strong>Results</strong>\n",
        "\n",
        "*   Training set: 93.7%\n",
        "*   Testing set: 93.5%\n",
        "*   Less acurate than using mean squared error but significant faster to compute\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGTSQQ7uDuDB"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    correct = 0\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    correct /= size\n",
        "    print(f\"Training Accuracy: {(100*correct):>0.1f}%\")"
      ],
      "execution_count": 450,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKKuMvwCe7Ov"
      },
      "source": [
        "def test(dataloader, model):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for  batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    correct /= size\n",
        "    print(f\"Test Accuracy: {(100*correct):>0.1f}%\")"
      ],
      "execution_count": 451,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa8Qeep7fHY6"
      },
      "source": [
        "sequence = OrderedDict([\n",
        "            ('linear1', nn.Linear(784, 30)),\n",
        "            ('sigmoid1', nn.Sigmoid()),\n",
        "            ('linear2', nn.Linear(30, 10)),\n",
        "            ('sigmoid2', nn.Sigmoid())\n",
        "])\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = False, batch_size = 10)\n",
        "model = NeuralNetwork(sequence).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 452,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQHWFbU2fedN",
        "outputId": "ce9e5ecd-6627-48c7-bd90-432ff9a7f24b"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    print(\"-------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)"
      ],
      "execution_count": 453,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------\n",
            "Training Accuracy: 83.8%\n",
            "Test Accuracy: 90.2%\n",
            "\n",
            "Epoch 2\n",
            "-------\n",
            "Training Accuracy: 91.4%\n",
            "Test Accuracy: 92.2%\n",
            "\n",
            "Epoch 3\n",
            "-------\n",
            "Training Accuracy: 92.5%\n",
            "Test Accuracy: 92.8%\n",
            "\n",
            "Epoch 4\n",
            "-------\n",
            "Training Accuracy: 93.2%\n",
            "Test Accuracy: 93.2%\n",
            "\n",
            "Epoch 5\n",
            "-------\n",
            "Training Accuracy: 93.7%\n",
            "Test Accuracy: 93.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlen7_D8vEtv"
      },
      "source": [
        "<h2>3 - Cross entrophy loss function, ReLU activation</h2>\n",
        "\n",
        "*   Input layer: 784 nodes\n",
        "*   Hidden layer: 30 nodes\n",
        "*   Output layer: 10 nodes\n",
        "*   Activation between nodes: relu\n",
        "*   Output activation: relu\n",
        "*   Loss function: cross-entropy\n",
        "*   Stochastic gradient descent 10 mini-batches\n",
        "*   5 epochs, learning rate = 0.3\n",
        "\n",
        "<strong>Results</strong>\n",
        "\n",
        "*   Training set: 95.4%\n",
        "*   Testing set: 94.9%\n",
        "*   Slightly accurate than the previous sigmoid experiment\n",
        "*   Results close to the first experiment using mean squared error\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cdMBhw-v3Xc"
      },
      "source": [
        "sequence = OrderedDict([\n",
        "            ('linear1', nn.Linear(784, 30)),\n",
        "            ('relu1', nn.ReLU()),\n",
        "            ('linear2', nn.Linear(30, 10)),\n",
        "            ('relu2', nn.ReLU())\n",
        "])\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = False, batch_size = 10)\n",
        "model = NeuralNetwork(sequence).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 458,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv6LSISAwLla",
        "outputId": "1caa2afd-3a69-4a7c-d0f8-6341a24b17b9"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    print(\"-------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)"
      ],
      "execution_count": 459,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------\n",
            "Training Accuracy: 79.4%\n",
            "Test Accuracy: 93.6%\n",
            "\n",
            "Epoch 2\n",
            "-------\n",
            "Training Accuracy: 93.6%\n",
            "Test Accuracy: 93.0%\n",
            "\n",
            "Epoch 3\n",
            "-------\n",
            "Training Accuracy: 94.6%\n",
            "Test Accuracy: 94.7%\n",
            "\n",
            "Epoch 4\n",
            "-------\n",
            "Training Accuracy: 95.2%\n",
            "Test Accuracy: 95.3%\n",
            "\n",
            "Epoch 5\n",
            "-------\n",
            "Training Accuracy: 95.4%\n",
            "Test Accuracy: 94.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqSI0fLUxCFP"
      },
      "source": [
        "<h2>4 - With batch size increased from 10 to 64</h2>\n",
        "\n",
        "*   Input layer: 784 nodes\n",
        "*   Hidden layer: 30 nodes\n",
        "*   Output layer: 10 nodes\n",
        "*   Activation between nodes: relu\n",
        "*   Output activation: relu\n",
        "*   Loss function: cross-entropy\n",
        "*   Stochastic gradient descent 64 mini-batches\n",
        "*   5 epochs, learning rate = 0.3\n",
        "\n",
        "<strong>Results</strong>\n",
        "\n",
        "*   Training set: 96.8%\n",
        "*   Testing set: 96.0%\n",
        "*   Both accuracy and performance increased\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgxLZamHxcw3"
      },
      "source": [
        "sequence = OrderedDict([\n",
        "            ('linear1', nn.Linear(784, 30)),\n",
        "            ('relu1', nn.ReLU()),\n",
        "            ('linear2', nn.Linear(30, 10)),\n",
        "            ('relu2', nn.ReLU())\n",
        "])\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = False, batch_size = 64)\n",
        "model = NeuralNetwork(sequence).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 466,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEYVa5TTxf5o",
        "outputId": "498124a4-bab8-4b18-fba2-1e927a8900f5"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    print(\"-------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)"
      ],
      "execution_count": 467,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------\n",
            "Training Accuracy: 76.3%\n",
            "Test Accuracy: 84.8%\n",
            "\n",
            "Epoch 2\n",
            "-------\n",
            "Training Accuracy: 93.3%\n",
            "Test Accuracy: 94.3%\n",
            "\n",
            "Epoch 3\n",
            "-------\n",
            "Training Accuracy: 95.7%\n",
            "Test Accuracy: 95.1%\n",
            "\n",
            "Epoch 4\n",
            "-------\n",
            "Training Accuracy: 96.4%\n",
            "Test Accuracy: 95.5%\n",
            "\n",
            "Epoch 5\n",
            "-------\n",
            "Training Accuracy: 96.8%\n",
            "Test Accuracy: 96.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLsaX2WHzKnd"
      },
      "source": [
        "<h2>5 - Add softmax to the output layer</h2>\n",
        "\n",
        "*   Input layer: 784 nodes\n",
        "*   Hidden layer: 30 nodes\n",
        "*   Output layer: 10 nodes\n",
        "*   Activation between nodes: relu\n",
        "*   Output activation: softmax\n",
        "*   Loss function: cross-entropy\n",
        "*   Stochastic gradient descent 64 mini-batches\n",
        "*   5 epochs, learning rate = 0.3\n",
        "\n",
        "<strong>Results</strong>\n",
        "\n",
        "*   Training set: 85.4%\n",
        "*   Testing set: 85.1%\n",
        "*   Accuracy dropped by 10%. Results are unstable (?)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajSNdoFNzUlB"
      },
      "source": [
        "sequence = OrderedDict([\n",
        "            ('linear1', nn.Linear(784, 30)),\n",
        "            ('relu1', nn.ReLU()),\n",
        "            ('linear2', nn.Linear(30, 10)),\n",
        "            ('softmax', nn.Softmax(dim=1))\n",
        "])\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = False, batch_size = 64)\n",
        "model = NeuralNetwork(sequence).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 478,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjpcNg7zzUVG",
        "outputId": "dbf5eeb6-fd1b-48f5-f93c-d9089b3142a7"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    print(\"-------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)"
      ],
      "execution_count": 479,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------\n",
            "Training Accuracy: 68.6%\n",
            "Test Accuracy: 82.0%\n",
            "\n",
            "Epoch 2\n",
            "-------\n",
            "Training Accuracy: 83.1%\n",
            "Test Accuracy: 83.6%\n",
            "\n",
            "Epoch 3\n",
            "-------\n",
            "Training Accuracy: 84.3%\n",
            "Test Accuracy: 84.2%\n",
            "\n",
            "Epoch 4\n",
            "-------\n",
            "Training Accuracy: 84.9%\n",
            "Test Accuracy: 84.8%\n",
            "\n",
            "Epoch 5\n",
            "-------\n",
            "Training Accuracy: 85.4%\n",
            "Test Accuracy: 85.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jFVUMR71G58"
      },
      "source": [
        "<h2>6 - Add more hidden layers</h2>\n",
        "\n",
        "*   Input layer: 784 nodes\n",
        "*   Hidden layer: 128 nodes\n",
        "*   Hidden layer 2: 64 nodes\n",
        "*   Output layer: 10 nodes\n",
        "*   Activation between nodes: relu\n",
        "*   Output activation: relu\n",
        "*   Loss function: cross-entropy\n",
        "*   Stochastic gradient descent 64 mini-batches\n",
        "*   5 epochs, learning rate = 0.3\n",
        "\n",
        "<strong>Results</strong>\n",
        "\n",
        "*   Training set: 98.7%\n",
        "*   Testing set: 97.7%\n",
        "*   Best results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0HJPqj81Gg9"
      },
      "source": [
        "sequence = OrderedDict([\n",
        "            ('linear1', nn.Linear(784, 128)),\n",
        "            ('relu1', nn.ReLU()),\n",
        "            ('linear2', nn.Linear(128, 64)),\n",
        "            ('relu2', nn.ReLU()),\n",
        "            ('linear3', nn.Linear(64, 10)),\n",
        "            ('relu3', nn.ReLU())\n",
        "])\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = False, batch_size = 64)\n",
        "model = NeuralNetwork(sequence).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 485,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1geN_eq91TP1",
        "outputId": "a5b0f016-96e5-43f3-9230-d01e44aca258"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    print(\"-------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)"
      ],
      "execution_count": 486,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------\n",
            "Training Accuracy: 80.2%\n",
            "Test Accuracy: 83.8%\n",
            "\n",
            "Epoch 2\n",
            "-------\n",
            "Training Accuracy: 94.4%\n",
            "Test Accuracy: 95.9%\n",
            "\n",
            "Epoch 3\n",
            "-------\n",
            "Training Accuracy: 97.6%\n",
            "Test Accuracy: 97.0%\n",
            "\n",
            "Epoch 4\n",
            "-------\n",
            "Training Accuracy: 98.3%\n",
            "Test Accuracy: 97.5%\n",
            "\n",
            "Epoch 5\n",
            "-------\n",
            "Training Accuracy: 98.7%\n",
            "Test Accuracy: 97.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQZ-dZTI2aFM"
      },
      "source": [
        "<h2>7 - Hypertuning / Regularization</h2>\n",
        "\n",
        "*   Input layer: 784 nodes\n",
        "*   Hidden layer: 128 nodes\n",
        "*   Hidden layer 2: 64 nodes\n",
        "*   Output layer: 10 nodes\n",
        "*   Activation between nodes: relu\n",
        "*   Output activation: relu\n",
        "*   Loss function: cross-entropy\n",
        "*   Stochastic gradient descent 64 mini-batches\n",
        "*   5 epochs, learning rate = 0.3\n",
        "*   Added dropout on the first and second layer with 20% or probability\n",
        "\n",
        "<strong>Results</strong>\n",
        "\n",
        "*   Training set: 96.9%\n",
        "*   Testing set: 96.2%\n",
        "*   Tried adding momentum and weight decay but had very poor results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VU-04ar3FAm"
      },
      "source": [
        "sequence = OrderedDict([\n",
        "            ('linear1', nn.Linear(784, 128)),\n",
        "            ('dropout1', nn.Dropout(0.2)),\n",
        "            ('relu1', nn.ReLU()),\n",
        "            ('linear2', nn.Linear(128, 64)),\n",
        "            ('dropout2', nn.Dropout(0.2)),            \n",
        "            ('relu2', nn.ReLU()),\n",
        "            ('linear3', nn.Linear(64, 10)),\n",
        "            ('relu3', nn.ReLU())\n",
        "])\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = False, batch_size = 64)\n",
        "model = NeuralNetwork(sequence).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) # weight_decay = 0.1, momentum = 0.5\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 505,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6PcRC8A3VHS",
        "outputId": "65f4c310-8943-4a16-b570-6ad1580ddd85"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    print(\"-------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)"
      ],
      "execution_count": 506,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------\n",
            "Training Accuracy: 85.8%\n",
            "Test Accuracy: 92.9%\n",
            "\n",
            "Epoch 2\n",
            "-------\n",
            "Training Accuracy: 94.7%\n",
            "Test Accuracy: 94.7%\n",
            "\n",
            "Epoch 3\n",
            "-------\n",
            "Training Accuracy: 95.9%\n",
            "Test Accuracy: 95.4%\n",
            "\n",
            "Epoch 4\n",
            "-------\n",
            "Training Accuracy: 96.4%\n",
            "Test Accuracy: 95.5%\n",
            "\n",
            "Epoch 5\n",
            "-------\n",
            "Training Accuracy: 96.9%\n",
            "Test Accuracy: 95.7%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}