{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Optimization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdzkNXsnuT9fj3avtCXsoL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3tHPxSEPvWo",
        "outputId": "dd8ce40b-c43e-4494-820e-77a5b23351d8"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision\n",
        "from collections import OrderedDict\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, sequence):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.nn_stack = nn.Sequential(sequence)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        return self.nn_stack(x)\n",
        "\n",
        "def one_hot(y):\n",
        "    one_hot = torch.zeros([1, 10])\n",
        "    one_hot[0][y] = 1\n",
        "    return one_hot\n",
        "\n",
        "def load_data(one_hot_transform = False, batch_size = 10):\n",
        "    transformation = None\n",
        "    if one_hot_transform:\n",
        "        transformation = Lambda(lambda y: one_hot(y))\n",
        "    \n",
        "    training_data = datasets.MNIST(\n",
        "        root=\"data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=ToTensor(),\n",
        "        target_transform=transformation\n",
        "    )\n",
        "\n",
        "    test_data = datasets.MNIST(\n",
        "        root=\"data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=ToTensor(),\n",
        "        target_transform=transformation\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
        "    test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
        "\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using\", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaYVHc8fQNjj"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer, show_info = True):\n",
        "    correct = 0\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    correct /= size\n",
        "    if show_info:\n",
        "      print(f\"Training Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n",
        "    return correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91vj_fb0QUhq"
      },
      "source": [
        "def test(dataloader, model, show_info = True):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for  batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    correct /= size\n",
        "    if show_info:\n",
        "      print(f\"Test Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n",
        "    return correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-yzcbYIm5ZU"
      },
      "source": [
        "<h2>Without hyperparameter tuning</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2-CrSpyQV26"
      },
      "source": [
        "# OrderedDict([('input', Linear(in_features=784, out_features=30, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=30, out_features=30, bias=True)), ('relu0', ReLU()), ('output', Linear(in_features=30, out_features=10, bias=True))])\n",
        "\n",
        "sequence = OrderedDict([\n",
        "            ('linear1', nn.Linear(784, 30)),\n",
        "            ('relu1', nn.ReLU()),\n",
        "            ('linear2', nn.Linear(30, 30)),\n",
        "            ('relu2', nn.ReLU()),\n",
        "            ('linear3', nn.Linear(30, 10))\n",
        "#            ('soft', nn.Softmax(dim=1))\n",
        "])\n",
        "\n",
        "epochs = 10\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = False, batch_size = 64)\n",
        "model = NeuralNetwork(sequence).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "qlsKD162Qjyr",
        "outputId": "5a9cc396-4be7-4c61-e1f8-2f29aedb065e"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    print(\"-------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------\n",
            "Training Accuracy: 87.9%\n",
            "Test Accuracy: 93.0%\n",
            "\n",
            "Epoch 2\n",
            "-------\n",
            "Training Accuracy: 94.8%\n",
            "Test Accuracy: 94.6%\n",
            "\n",
            "Epoch 3\n",
            "-------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-da2903e9c012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {t+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-6d8f7de0d090>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, show_info)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsw3f3cBomLA"
      },
      "source": [
        "<h3>Conclusions</h3>\n",
        "\n",
        "It seems the model is overfitting after epoch 27. The trainning accuracy increases but the testing accuracy starts to decrease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew0pnu4gqnvY"
      },
      "source": [
        "<h2>L2 Regularization</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTTp8ubAo13B"
      },
      "source": [
        "sequence = OrderedDict([\n",
        "            ('linear1', nn.Linear(784, 128)),\n",
        "            ('relu1', nn.ReLU()),\n",
        "            ('linear2', nn.Linear(128, 64)),\n",
        "            ('relu2', nn.ReLU()),\n",
        "            ('linear3', nn.Linear(64, 10))\n",
        "])\n",
        "\n",
        "epochs = 50\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = False, batch_size = 64)\n",
        "model = NeuralNetwork(sequence).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, weight_decay = 0.01) # weight_decay = 0.1, momentum = 0.5\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QQfw8ffo7lD",
        "outputId": "cf14bd23-7646-4d6e-9710-848c1dcb4b53"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    print(\"-------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------\n",
            "Training Accuracy: 86.4%\n",
            "Test Accuracy: 90.7%\n",
            "\n",
            "Epoch 2\n",
            "-------\n",
            "Training Accuracy: 91.7%\n",
            "Test Accuracy: 91.3%\n",
            "\n",
            "Epoch 3\n",
            "-------\n",
            "Training Accuracy: 92.1%\n",
            "Test Accuracy: 91.2%\n",
            "\n",
            "Epoch 4\n",
            "-------\n",
            "Training Accuracy: 92.4%\n",
            "Test Accuracy: 90.8%\n",
            "\n",
            "Epoch 5\n",
            "-------\n",
            "Training Accuracy: 92.5%\n",
            "Test Accuracy: 91.2%\n",
            "\n",
            "Epoch 6\n",
            "-------\n",
            "Training Accuracy: 92.6%\n",
            "Test Accuracy: 91.3%\n",
            "\n",
            "Epoch 7\n",
            "-------\n",
            "Training Accuracy: 92.6%\n",
            "Test Accuracy: 91.5%\n",
            "\n",
            "Epoch 8\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.6%\n",
            "\n",
            "Epoch 9\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.8%\n",
            "\n",
            "Epoch 10\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.9%\n",
            "\n",
            "Epoch 11\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.4%\n",
            "\n",
            "Epoch 12\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.8%\n",
            "\n",
            "Epoch 13\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.8%\n",
            "\n",
            "Epoch 14\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.8%\n",
            "\n",
            "Epoch 15\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.9%\n",
            "\n",
            "Epoch 16\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.9%\n",
            "\n",
            "Epoch 17\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.8%\n",
            "\n",
            "Epoch 18\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.9%\n",
            "\n",
            "Epoch 19\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.9%\n",
            "\n",
            "Epoch 20\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.8%\n",
            "\n",
            "Epoch 21\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.8%\n",
            "\n",
            "Epoch 22\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 92.0%\n",
            "\n",
            "Epoch 23\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.6%\n",
            "\n",
            "Epoch 24\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 92.1%\n",
            "\n",
            "Epoch 25\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.7%\n",
            "\n",
            "Epoch 26\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 92.0%\n",
            "\n",
            "Epoch 27\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.8%\n",
            "\n",
            "Epoch 28\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.9%\n",
            "\n",
            "Epoch 29\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.9%\n",
            "\n",
            "Epoch 30\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.3%\n",
            "\n",
            "Epoch 31\n",
            "-------\n",
            "Training Accuracy: 92.6%\n",
            "Test Accuracy: 91.6%\n",
            "\n",
            "Epoch 32\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.5%\n",
            "\n",
            "Epoch 33\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 92.0%\n",
            "\n",
            "Epoch 34\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.7%\n",
            "\n",
            "Epoch 35\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.9%\n",
            "\n",
            "Epoch 36\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.9%\n",
            "\n",
            "Epoch 37\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 92.0%\n",
            "\n",
            "Epoch 38\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 92.1%\n",
            "\n",
            "Epoch 39\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 92.0%\n",
            "\n",
            "Epoch 40\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.8%\n",
            "\n",
            "Epoch 41\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 92.0%\n",
            "\n",
            "Epoch 42\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.9%\n",
            "\n",
            "Epoch 43\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.6%\n",
            "\n",
            "Epoch 44\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.9%\n",
            "\n",
            "Epoch 45\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.7%\n",
            "\n",
            "Epoch 46\n",
            "-------\n",
            "Training Accuracy: 92.7%\n",
            "Test Accuracy: 91.7%\n",
            "\n",
            "Epoch 47\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.7%\n",
            "\n",
            "Epoch 48\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.7%\n",
            "\n",
            "Epoch 49\n",
            "-------\n",
            "Training Accuracy: 92.8%\n",
            "Test Accuracy: 91.2%\n",
            "\n",
            "Epoch 50\n",
            "-------\n",
            "Training Accuracy: 92.6%\n",
            "Test Accuracy: 91.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPeT9CLwsNxP"
      },
      "source": [
        "<h2>Dropout in a deep neural network</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Plg8_JVKEw"
      },
      "source": [
        "sequence = OrderedDict([\n",
        "            ('linear1', nn.Linear(784, 128)),\n",
        "            ('relu1', nn.ReLU()),\n",
        "            ('linear2', nn.Linear(128, 128)),\n",
        "            ('dropout', nn.Dropout(0.2)),\n",
        "            ('relu2', nn.ReLU()),\n",
        "            ('linear3', nn.Linear(128, 10))\n",
        "])\n",
        "\n",
        "epochs = 50\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = False, batch_size = 64)\n",
        "model = NeuralNetwork(sequence).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.6)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3WQ2CI9rU8i",
        "outputId": "a2372009-e345-42e4-fcc8-173e278de101"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    print(\"-------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------\n",
            "Training Accuracy: 90.9%\n",
            "Test Accuracy: 94.9%\n",
            "\n",
            "Epoch 2\n",
            "-------\n",
            "Training Accuracy: 96.1%\n",
            "Test Accuracy: 95.3%\n",
            "\n",
            "Epoch 3\n",
            "-------\n",
            "Training Accuracy: 97.0%\n",
            "Test Accuracy: 96.0%\n",
            "\n",
            "Epoch 4\n",
            "-------\n",
            "Training Accuracy: 97.6%\n",
            "Test Accuracy: 96.6%\n",
            "\n",
            "Epoch 5\n",
            "-------\n",
            "Training Accuracy: 97.8%\n",
            "Test Accuracy: 96.1%\n",
            "\n",
            "Epoch 6\n",
            "-------\n",
            "Training Accuracy: 98.1%\n",
            "Test Accuracy: 96.5%\n",
            "\n",
            "Epoch 7\n",
            "-------\n",
            "Training Accuracy: 98.3%\n",
            "Test Accuracy: 96.1%\n",
            "\n",
            "Epoch 8\n",
            "-------\n",
            "Training Accuracy: 98.5%\n",
            "Test Accuracy: 97.0%\n",
            "\n",
            "Epoch 9\n",
            "-------\n",
            "Training Accuracy: 98.4%\n",
            "Test Accuracy: 97.2%\n",
            "\n",
            "Epoch 10\n",
            "-------\n",
            "Training Accuracy: 98.7%\n",
            "Test Accuracy: 97.3%\n",
            "\n",
            "Epoch 11\n",
            "-------\n",
            "Training Accuracy: 98.7%\n",
            "Test Accuracy: 97.0%\n",
            "\n",
            "Epoch 12\n",
            "-------\n",
            "Training Accuracy: 98.8%\n",
            "Test Accuracy: 96.5%\n",
            "\n",
            "Epoch 13\n",
            "-------\n",
            "Training Accuracy: 98.8%\n",
            "Test Accuracy: 97.5%\n",
            "\n",
            "Epoch 14\n",
            "-------\n",
            "Training Accuracy: 99.0%\n",
            "Test Accuracy: 96.8%\n",
            "\n",
            "Epoch 15\n",
            "-------\n",
            "Training Accuracy: 98.9%\n",
            "Test Accuracy: 97.0%\n",
            "\n",
            "Epoch 16\n",
            "-------\n",
            "Training Accuracy: 98.9%\n",
            "Test Accuracy: 97.2%\n",
            "\n",
            "Epoch 17\n",
            "-------\n",
            "Training Accuracy: 98.9%\n",
            "Test Accuracy: 97.1%\n",
            "\n",
            "Epoch 18\n",
            "-------\n",
            "Training Accuracy: 99.0%\n",
            "Test Accuracy: 97.3%\n",
            "\n",
            "Epoch 19\n",
            "-------\n",
            "Training Accuracy: 99.1%\n",
            "Test Accuracy: 97.0%\n",
            "\n",
            "Epoch 20\n",
            "-------\n",
            "Training Accuracy: 99.2%\n",
            "Test Accuracy: 96.9%\n",
            "\n",
            "Epoch 21\n",
            "-------\n",
            "Training Accuracy: 99.0%\n",
            "Test Accuracy: 97.2%\n",
            "\n",
            "Epoch 22\n",
            "-------\n",
            "Training Accuracy: 99.1%\n",
            "Test Accuracy: 97.4%\n",
            "\n",
            "Epoch 23\n",
            "-------\n",
            "Training Accuracy: 99.2%\n",
            "Test Accuracy: 97.2%\n",
            "\n",
            "Epoch 24\n",
            "-------\n",
            "Training Accuracy: 99.1%\n",
            "Test Accuracy: 96.5%\n",
            "\n",
            "Epoch 25\n",
            "-------\n",
            "Training Accuracy: 99.0%\n",
            "Test Accuracy: 97.6%\n",
            "\n",
            "Epoch 26\n",
            "-------\n",
            "Training Accuracy: 99.3%\n",
            "Test Accuracy: 97.2%\n",
            "\n",
            "Epoch 27\n",
            "-------\n",
            "Training Accuracy: 99.4%\n",
            "Test Accuracy: 97.5%\n",
            "\n",
            "Epoch 28\n",
            "-------\n",
            "Training Accuracy: 99.3%\n",
            "Test Accuracy: 97.5%\n",
            "\n",
            "Epoch 29\n",
            "-------\n",
            "Training Accuracy: 99.1%\n",
            "Test Accuracy: 97.0%\n",
            "\n",
            "Epoch 30\n",
            "-------\n",
            "Training Accuracy: 99.0%\n",
            "Test Accuracy: 97.1%\n",
            "\n",
            "Epoch 31\n",
            "-------\n",
            "Training Accuracy: 99.2%\n",
            "Test Accuracy: 97.5%\n",
            "\n",
            "Epoch 32\n",
            "-------\n",
            "Training Accuracy: 99.2%\n",
            "Test Accuracy: 97.4%\n",
            "\n",
            "Epoch 33\n",
            "-------\n",
            "Training Accuracy: 99.1%\n",
            "Test Accuracy: 97.4%\n",
            "\n",
            "Epoch 34\n",
            "-------\n",
            "Training Accuracy: 99.2%\n",
            "Test Accuracy: 97.6%\n",
            "\n",
            "Epoch 35\n",
            "-------\n",
            "Training Accuracy: 99.3%\n",
            "Test Accuracy: 97.4%\n",
            "\n",
            "Epoch 36\n",
            "-------\n",
            "Training Accuracy: 99.4%\n",
            "Test Accuracy: 97.6%\n",
            "\n",
            "Epoch 37\n",
            "-------\n",
            "Training Accuracy: 99.4%\n",
            "Test Accuracy: 97.7%\n",
            "\n",
            "Epoch 38\n",
            "-------\n",
            "Training Accuracy: 99.4%\n",
            "Test Accuracy: 97.5%\n",
            "\n",
            "Epoch 39\n",
            "-------\n",
            "Training Accuracy: 99.3%\n",
            "Test Accuracy: 97.5%\n",
            "\n",
            "Epoch 40\n",
            "-------\n",
            "Training Accuracy: 99.4%\n",
            "Test Accuracy: 97.7%\n",
            "\n",
            "Epoch 41\n",
            "-------\n",
            "Training Accuracy: 99.3%\n",
            "Test Accuracy: 97.4%\n",
            "\n",
            "Epoch 42\n",
            "-------\n",
            "Training Accuracy: 99.3%\n",
            "Test Accuracy: 97.5%\n",
            "\n",
            "Epoch 43\n",
            "-------\n",
            "Training Accuracy: 99.3%\n",
            "Test Accuracy: 97.5%\n",
            "\n",
            "Epoch 44\n",
            "-------\n",
            "Training Accuracy: 99.3%\n",
            "Test Accuracy: 97.6%\n",
            "\n",
            "Epoch 45\n",
            "-------\n",
            "Training Accuracy: 99.4%\n",
            "Test Accuracy: 96.3%\n",
            "\n",
            "Epoch 46\n",
            "-------\n",
            "Training Accuracy: 99.5%\n",
            "Test Accuracy: 97.6%\n",
            "\n",
            "Epoch 47\n",
            "-------\n",
            "Training Accuracy: 99.3%\n",
            "Test Accuracy: 97.5%\n",
            "\n",
            "Epoch 48\n",
            "-------\n",
            "Training Accuracy: 99.1%\n",
            "Test Accuracy: 97.1%\n",
            "\n",
            "Epoch 49\n",
            "-------\n",
            "Training Accuracy: 99.3%\n",
            "Test Accuracy: 97.4%\n",
            "\n",
            "Epoch 50\n",
            "-------\n",
            "Training Accuracy: 99.3%\n",
            "Test Accuracy: 97.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE-RPlAWpBdw"
      },
      "source": [
        "<h1>Deeper Neural Network with Dropout</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxw-poo8rVsU"
      },
      "source": [
        "sequence = OrderedDict([\n",
        "            ('linear1', nn.Linear(784, 256)),\n",
        "            ('relu1', nn.ReLU()),\n",
        "            ('linear2', nn.Linear(256, 256)),\n",
        "            ('dropout2', nn.Dropout(0.8)),\n",
        "            ('relu2', nn.ReLU()),\n",
        "            ('linear3', nn.Linear(256, 128)),\n",
        "            ('dropout3', nn.Dropout(0.8)),\n",
        "            ('relu3', nn.ReLU()),\n",
        "            ('linear4', nn.Linear(128, 128)),\n",
        "            ('dropout4', nn.Dropout(0.8)),\n",
        "            ('relu4', nn.ReLU()),\n",
        "            ('linear5', nn.Linear(128, 64)),\n",
        "            ('dropout5', nn.Dropout(0.8)),\n",
        "            ('relu5', nn.ReLU()),\n",
        "            ('linear6', nn.Linear(64, 64)),\n",
        "            ('dropout6', nn.Dropout(0.8)),\n",
        "            ('relu6', nn.ReLU()),\n",
        "            ('linear7', nn.Linear(64, 32)),\n",
        "            ('dropout7', nn.Dropout(0.8)),\n",
        "            ('relu7', nn.ReLU()),\n",
        "            ('linear8', nn.Linear(32, 32)),\n",
        "            ('dropout8', nn.Dropout(0.8)),\n",
        "            ('relu8', nn.ReLU()),\n",
        "            ('linear9', nn.Linear(32, 10)),\n",
        "            ('relu9', nn.ReLU()),\n",
        "\n",
        "])\n",
        "\n",
        "epochs = 10\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = False, batch_size = 64)\n",
        "model = NeuralNetwork(sequence).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.6)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of2RGFqjpAEL",
        "outputId": "5bd9ab73-e15e-42ab-c8fb-c6ff5c98450a"
      },
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    print(\"-------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------\n",
            "Training Accuracy: 11.0%\n",
            "Test Accuracy: 9.8%\n",
            "\n",
            "Epoch 2\n",
            "-------\n",
            "Training Accuracy: 9.9%\n",
            "Test Accuracy: 9.8%\n",
            "\n",
            "Epoch 3\n",
            "-------\n",
            "Training Accuracy: 9.9%\n",
            "Test Accuracy: 9.8%\n",
            "\n",
            "Epoch 4\n",
            "-------\n",
            "Training Accuracy: 9.9%\n",
            "Test Accuracy: 9.8%\n",
            "\n",
            "Epoch 5\n",
            "-------\n",
            "Training Accuracy: 9.9%\n",
            "Test Accuracy: 9.8%\n",
            "\n",
            "Epoch 6\n",
            "-------\n",
            "Training Accuracy: 9.9%\n",
            "Test Accuracy: 9.8%\n",
            "\n",
            "Epoch 7\n",
            "-------\n",
            "Training Accuracy: 9.9%\n",
            "Test Accuracy: 9.8%\n",
            "\n",
            "Epoch 8\n",
            "-------\n",
            "Training Accuracy: 9.9%\n",
            "Test Accuracy: 9.8%\n",
            "\n",
            "Epoch 9\n",
            "-------\n",
            "Training Accuracy: 9.9%\n",
            "Test Accuracy: 9.8%\n",
            "\n",
            "Epoch 10\n",
            "-------\n",
            "Training Accuracy: 9.9%\n",
            "Test Accuracy: 9.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m02cxeysmKg"
      },
      "source": [
        "<h1>Deeper Neural Network Experiments</h1>\n",
        "This experiment increases the number of layers using a different number of nodes. The code try the following combination:\n",
        "\n",
        "\n",
        "*   From 1 to 10 hidden layers\n",
        "*   Using 10, 30, 60, 100 and 200 nodes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqWivqKgJQE5",
        "outputId": "3505f222-4568-44fc-bd4d-2c228067feae"
      },
      "source": [
        "def build_sequence(hidden_layers, nodes):\n",
        "  # input\n",
        "  sequence = OrderedDict([('input', nn.Linear(784, nodes)),\n",
        "                          ('input_activation', nn.ReLU())])\n",
        "  for i in range(hidden_layers):\n",
        "    sequence['linear' + str(i)] = nn.Linear(nodes, nodes)\n",
        "    sequence['relu' + str(i)] = nn.ReLU()\n",
        "\n",
        "  # output\n",
        "  sequence['output'] = nn.Linear(nodes, 10)\n",
        "\n",
        "  return sequence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('input', Linear(in_features=784, out_features=10, bias=True)),\n",
              "             ('input_activation', ReLU()),\n",
              "             ('linear0', Linear(in_features=10, out_features=10, bias=True)),\n",
              "             ('relu0', ReLU()),\n",
              "             ('linear1', Linear(in_features=10, out_features=10, bias=True)),\n",
              "             ('relu1', ReLU()),\n",
              "             ('output', Linear(in_features=10, out_features=10, bias=True))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG-DN9WdqJb5"
      },
      "source": [
        "epochs = 10\n",
        "learning_rate = 0.3\n",
        "train_dataloader, test_dataloader = load_data(one_hot_transform = False, batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9P7WirvstsI",
        "outputId": "0f0a0869-c096-43c9-f8b7-9a9ffa859c36"
      },
      "source": [
        "results = []\n",
        "result = {}\n",
        "train_accuracy = 0\n",
        "test_accuracy = 0\n",
        "for n in [10, 30, 60, 100, 200]:\n",
        "  for x in range(10):\n",
        "    sequence = build_sequence(hidden_layers = x + 1, nodes = n)\n",
        "    print(sequence)\n",
        "    model = NeuralNetwork(sequence).to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for t in range(epochs):\n",
        "        train_accuracy = train(train_dataloader, model, loss_fn, optimizer, False)\n",
        "        test_accuracy = test(test_dataloader, model, False)\n",
        "    result = {'hidden_layers': x + 1, 'nodes': n, 'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy}\n",
        "    print(result)\n",
        "    results.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('input', Linear(in_features=784, out_features=10, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=10, out_features=10, bias=True)), ('relu0', ReLU()), ('output', Linear(in_features=10, out_features=10, bias=True))])\n",
            "{'hidden_layers': 1, 'nodes': 10, 'train_accuracy': 0.9396833333333333, 'test_accuracy': 0.9281}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=10, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=10, out_features=10, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=10, out_features=10, bias=True)), ('relu1', ReLU()), ('output', Linear(in_features=10, out_features=10, bias=True))])\n",
            "{'hidden_layers': 2, 'nodes': 10, 'train_accuracy': 0.9383, 'test_accuracy': 0.9277}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=10, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=10, out_features=10, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=10, out_features=10, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=10, out_features=10, bias=True)), ('relu2', ReLU()), ('output', Linear(in_features=10, out_features=10, bias=True))])\n",
            "{'hidden_layers': 3, 'nodes': 10, 'train_accuracy': 0.9367666666666666, 'test_accuracy': 0.9224}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=10, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=10, out_features=10, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=10, out_features=10, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=10, out_features=10, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=10, out_features=10, bias=True)), ('relu3', ReLU()), ('output', Linear(in_features=10, out_features=10, bias=True))])\n",
            "{'hidden_layers': 4, 'nodes': 10, 'train_accuracy': 0.9274333333333333, 'test_accuracy': 0.9114}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=10, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=10, out_features=10, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=10, out_features=10, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=10, out_features=10, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=10, out_features=10, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=10, out_features=10, bias=True)), ('relu4', ReLU()), ('output', Linear(in_features=10, out_features=10, bias=True))])\n",
            "{'hidden_layers': 5, 'nodes': 10, 'train_accuracy': 0.8065333333333333, 'test_accuracy': 0.8092}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=10, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=10, out_features=10, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=10, out_features=10, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=10, out_features=10, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=10, out_features=10, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=10, out_features=10, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=10, out_features=10, bias=True)), ('relu5', ReLU()), ('output', Linear(in_features=10, out_features=10, bias=True))])\n",
            "{'hidden_layers': 6, 'nodes': 10, 'train_accuracy': 0.10298333333333333, 'test_accuracy': 0.0958}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=10, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=10, out_features=10, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=10, out_features=10, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=10, out_features=10, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=10, out_features=10, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=10, out_features=10, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=10, out_features=10, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=10, out_features=10, bias=True)), ('relu6', ReLU()), ('output', Linear(in_features=10, out_features=10, bias=True))])\n",
            "{'hidden_layers': 7, 'nodes': 10, 'train_accuracy': 0.2021, 'test_accuracy': 0.206}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=10, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=10, out_features=10, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=10, out_features=10, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=10, out_features=10, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=10, out_features=10, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=10, out_features=10, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=10, out_features=10, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=10, out_features=10, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=10, out_features=10, bias=True)), ('relu7', ReLU()), ('output', Linear(in_features=10, out_features=10, bias=True))])\n",
            "{'hidden_layers': 8, 'nodes': 10, 'train_accuracy': 0.11155, 'test_accuracy': 0.1028}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=10, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=10, out_features=10, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=10, out_features=10, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=10, out_features=10, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=10, out_features=10, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=10, out_features=10, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=10, out_features=10, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=10, out_features=10, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=10, out_features=10, bias=True)), ('relu7', ReLU()), ('linear8', Linear(in_features=10, out_features=10, bias=True)), ('relu8', ReLU()), ('output', Linear(in_features=10, out_features=10, bias=True))])\n",
            "{'hidden_layers': 9, 'nodes': 10, 'train_accuracy': 0.11155, 'test_accuracy': 0.1028}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=10, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=10, out_features=10, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=10, out_features=10, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=10, out_features=10, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=10, out_features=10, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=10, out_features=10, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=10, out_features=10, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=10, out_features=10, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=10, out_features=10, bias=True)), ('relu7', ReLU()), ('linear8', Linear(in_features=10, out_features=10, bias=True)), ('relu8', ReLU()), ('linear9', Linear(in_features=10, out_features=10, bias=True)), ('relu9', ReLU()), ('output', Linear(in_features=10, out_features=10, bias=True))])\n",
            "{'hidden_layers': 10, 'nodes': 10, 'train_accuracy': 0.11155, 'test_accuracy': 0.1028}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=30, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=30, out_features=30, bias=True)), ('relu0', ReLU()), ('output', Linear(in_features=30, out_features=10, bias=True))])\n",
            "{'hidden_layers': 1, 'nodes': 30, 'train_accuracy': 0.9799666666666667, 'test_accuracy': 0.9637}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=30, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=30, out_features=30, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=30, out_features=30, bias=True)), ('relu1', ReLU()), ('output', Linear(in_features=30, out_features=10, bias=True))])\n",
            "{'hidden_layers': 2, 'nodes': 30, 'train_accuracy': 0.9768166666666667, 'test_accuracy': 0.9664}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=30, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=30, out_features=30, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=30, out_features=30, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=30, out_features=30, bias=True)), ('relu2', ReLU()), ('output', Linear(in_features=30, out_features=10, bias=True))])\n",
            "{'hidden_layers': 3, 'nodes': 30, 'train_accuracy': 0.97645, 'test_accuracy': 0.9606}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=30, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=30, out_features=30, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=30, out_features=30, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=30, out_features=30, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=30, out_features=30, bias=True)), ('relu3', ReLU()), ('output', Linear(in_features=30, out_features=10, bias=True))])\n",
            "{'hidden_layers': 4, 'nodes': 30, 'train_accuracy': 0.9747, 'test_accuracy': 0.9641}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=30, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=30, out_features=30, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=30, out_features=30, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=30, out_features=30, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=30, out_features=30, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=30, out_features=30, bias=True)), ('relu4', ReLU()), ('output', Linear(in_features=30, out_features=10, bias=True))])\n",
            "{'hidden_layers': 5, 'nodes': 30, 'train_accuracy': 0.9686666666666667, 'test_accuracy': 0.9524}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=30, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=30, out_features=30, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=30, out_features=30, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=30, out_features=30, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=30, out_features=30, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=30, out_features=30, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=30, out_features=30, bias=True)), ('relu5', ReLU()), ('output', Linear(in_features=30, out_features=10, bias=True))])\n",
            "{'hidden_layers': 6, 'nodes': 30, 'train_accuracy': 0.9668833333333333, 'test_accuracy': 0.9555}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=30, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=30, out_features=30, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=30, out_features=30, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=30, out_features=30, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=30, out_features=30, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=30, out_features=30, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=30, out_features=30, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=30, out_features=30, bias=True)), ('relu6', ReLU()), ('output', Linear(in_features=30, out_features=10, bias=True))])\n",
            "{'hidden_layers': 7, 'nodes': 30, 'train_accuracy': 0.9575333333333333, 'test_accuracy': 0.946}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=30, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=30, out_features=30, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=30, out_features=30, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=30, out_features=30, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=30, out_features=30, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=30, out_features=30, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=30, out_features=30, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=30, out_features=30, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=30, out_features=30, bias=True)), ('relu7', ReLU()), ('output', Linear(in_features=30, out_features=10, bias=True))])\n",
            "{'hidden_layers': 8, 'nodes': 30, 'train_accuracy': 0.33848333333333336, 'test_accuracy': 0.0958}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=30, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=30, out_features=30, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=30, out_features=30, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=30, out_features=30, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=30, out_features=30, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=30, out_features=30, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=30, out_features=30, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=30, out_features=30, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=30, out_features=30, bias=True)), ('relu7', ReLU()), ('linear8', Linear(in_features=30, out_features=30, bias=True)), ('relu8', ReLU()), ('output', Linear(in_features=30, out_features=10, bias=True))])\n",
            "{'hidden_layers': 9, 'nodes': 30, 'train_accuracy': 0.11155, 'test_accuracy': 0.1028}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=30, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=30, out_features=30, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=30, out_features=30, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=30, out_features=30, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=30, out_features=30, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=30, out_features=30, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=30, out_features=30, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=30, out_features=30, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=30, out_features=30, bias=True)), ('relu7', ReLU()), ('linear8', Linear(in_features=30, out_features=30, bias=True)), ('relu8', ReLU()), ('linear9', Linear(in_features=30, out_features=30, bias=True)), ('relu9', ReLU()), ('output', Linear(in_features=30, out_features=10, bias=True))])\n",
            "{'hidden_layers': 10, 'nodes': 30, 'train_accuracy': 0.11155, 'test_accuracy': 0.1028}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=60, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=60, out_features=60, bias=True)), ('relu0', ReLU()), ('output', Linear(in_features=60, out_features=10, bias=True))])\n",
            "{'hidden_layers': 1, 'nodes': 60, 'train_accuracy': 0.98905, 'test_accuracy': 0.9719}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=60, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=60, out_features=60, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=60, out_features=60, bias=True)), ('relu1', ReLU()), ('output', Linear(in_features=60, out_features=10, bias=True))])\n",
            "{'hidden_layers': 2, 'nodes': 60, 'train_accuracy': 0.98935, 'test_accuracy': 0.968}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=60, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=60, out_features=60, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=60, out_features=60, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=60, out_features=60, bias=True)), ('relu2', ReLU()), ('output', Linear(in_features=60, out_features=10, bias=True))])\n",
            "{'hidden_layers': 3, 'nodes': 60, 'train_accuracy': 0.9872, 'test_accuracy': 0.9719}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=60, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=60, out_features=60, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=60, out_features=60, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=60, out_features=60, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=60, out_features=60, bias=True)), ('relu3', ReLU()), ('output', Linear(in_features=60, out_features=10, bias=True))])\n",
            "{'hidden_layers': 4, 'nodes': 60, 'train_accuracy': 0.9840666666666666, 'test_accuracy': 0.9706}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=60, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=60, out_features=60, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=60, out_features=60, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=60, out_features=60, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=60, out_features=60, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=60, out_features=60, bias=True)), ('relu4', ReLU()), ('output', Linear(in_features=60, out_features=10, bias=True))])\n",
            "{'hidden_layers': 5, 'nodes': 60, 'train_accuracy': 0.9833166666666666, 'test_accuracy': 0.9701}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=60, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=60, out_features=60, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=60, out_features=60, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=60, out_features=60, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=60, out_features=60, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=60, out_features=60, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=60, out_features=60, bias=True)), ('relu5', ReLU()), ('output', Linear(in_features=60, out_features=10, bias=True))])\n",
            "{'hidden_layers': 6, 'nodes': 60, 'train_accuracy': 0.9796333333333334, 'test_accuracy': 0.9629}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=60, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=60, out_features=60, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=60, out_features=60, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=60, out_features=60, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=60, out_features=60, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=60, out_features=60, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=60, out_features=60, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=60, out_features=60, bias=True)), ('relu6', ReLU()), ('output', Linear(in_features=60, out_features=10, bias=True))])\n",
            "{'hidden_layers': 7, 'nodes': 60, 'train_accuracy': 0.9768833333333333, 'test_accuracy': 0.9669}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=60, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=60, out_features=60, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=60, out_features=60, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=60, out_features=60, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=60, out_features=60, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=60, out_features=60, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=60, out_features=60, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=60, out_features=60, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=60, out_features=60, bias=True)), ('relu7', ReLU()), ('output', Linear(in_features=60, out_features=10, bias=True))])\n",
            "{'hidden_layers': 8, 'nodes': 60, 'train_accuracy': 0.9683666666666667, 'test_accuracy': 0.9591}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=60, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=60, out_features=60, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=60, out_features=60, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=60, out_features=60, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=60, out_features=60, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=60, out_features=60, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=60, out_features=60, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=60, out_features=60, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=60, out_features=60, bias=True)), ('relu7', ReLU()), ('linear8', Linear(in_features=60, out_features=60, bias=True)), ('relu8', ReLU()), ('output', Linear(in_features=60, out_features=10, bias=True))])\n",
            "{'hidden_layers': 9, 'nodes': 60, 'train_accuracy': 0.11155, 'test_accuracy': 0.1028}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=60, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=60, out_features=60, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=60, out_features=60, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=60, out_features=60, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=60, out_features=60, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=60, out_features=60, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=60, out_features=60, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=60, out_features=60, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=60, out_features=60, bias=True)), ('relu7', ReLU()), ('linear8', Linear(in_features=60, out_features=60, bias=True)), ('relu8', ReLU()), ('linear9', Linear(in_features=60, out_features=60, bias=True)), ('relu9', ReLU()), ('output', Linear(in_features=60, out_features=10, bias=True))])\n",
            "{'hidden_layers': 10, 'nodes': 60, 'train_accuracy': 0.11155, 'test_accuracy': 0.1028}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=100, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=100, out_features=100, bias=True)), ('relu0', ReLU()), ('output', Linear(in_features=100, out_features=10, bias=True))])\n",
            "{'hidden_layers': 1, 'nodes': 100, 'train_accuracy': 0.99475, 'test_accuracy': 0.9752}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=100, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=100, out_features=100, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=100, out_features=100, bias=True)), ('relu1', ReLU()), ('output', Linear(in_features=100, out_features=10, bias=True))])\n",
            "{'hidden_layers': 2, 'nodes': 100, 'train_accuracy': 0.9913166666666666, 'test_accuracy': 0.9765}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=100, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=100, out_features=100, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=100, out_features=100, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=100, out_features=100, bias=True)), ('relu2', ReLU()), ('output', Linear(in_features=100, out_features=10, bias=True))])\n",
            "{'hidden_layers': 3, 'nodes': 100, 'train_accuracy': 0.99135, 'test_accuracy': 0.9764}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=100, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=100, out_features=100, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=100, out_features=100, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=100, out_features=100, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=100, out_features=100, bias=True)), ('relu3', ReLU()), ('output', Linear(in_features=100, out_features=10, bias=True))])\n",
            "{'hidden_layers': 4, 'nodes': 100, 'train_accuracy': 0.98865, 'test_accuracy': 0.9703}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=100, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=100, out_features=100, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=100, out_features=100, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=100, out_features=100, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=100, out_features=100, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=100, out_features=100, bias=True)), ('relu4', ReLU()), ('output', Linear(in_features=100, out_features=10, bias=True))])\n",
            "{'hidden_layers': 5, 'nodes': 100, 'train_accuracy': 0.98745, 'test_accuracy': 0.972}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=100, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=100, out_features=100, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=100, out_features=100, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=100, out_features=100, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=100, out_features=100, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=100, out_features=100, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=100, out_features=100, bias=True)), ('relu5', ReLU()), ('output', Linear(in_features=100, out_features=10, bias=True))])\n",
            "{'hidden_layers': 6, 'nodes': 100, 'train_accuracy': 0.9858833333333333, 'test_accuracy': 0.9673}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=100, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=100, out_features=100, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=100, out_features=100, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=100, out_features=100, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=100, out_features=100, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=100, out_features=100, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=100, out_features=100, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=100, out_features=100, bias=True)), ('relu6', ReLU()), ('output', Linear(in_features=100, out_features=10, bias=True))])\n",
            "{'hidden_layers': 7, 'nodes': 100, 'train_accuracy': 0.9806666666666667, 'test_accuracy': 0.9725}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=100, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=100, out_features=100, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=100, out_features=100, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=100, out_features=100, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=100, out_features=100, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=100, out_features=100, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=100, out_features=100, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=100, out_features=100, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=100, out_features=100, bias=True)), ('relu7', ReLU()), ('output', Linear(in_features=100, out_features=10, bias=True))])\n",
            "{'hidden_layers': 8, 'nodes': 100, 'train_accuracy': 0.9597666666666667, 'test_accuracy': 0.9564}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=100, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=100, out_features=100, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=100, out_features=100, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=100, out_features=100, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=100, out_features=100, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=100, out_features=100, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=100, out_features=100, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=100, out_features=100, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=100, out_features=100, bias=True)), ('relu7', ReLU()), ('linear8', Linear(in_features=100, out_features=100, bias=True)), ('relu8', ReLU()), ('output', Linear(in_features=100, out_features=10, bias=True))])\n",
            "{'hidden_layers': 9, 'nodes': 100, 'train_accuracy': 0.11155, 'test_accuracy': 0.1028}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=100, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=100, out_features=100, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=100, out_features=100, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=100, out_features=100, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=100, out_features=100, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=100, out_features=100, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=100, out_features=100, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=100, out_features=100, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=100, out_features=100, bias=True)), ('relu7', ReLU()), ('linear8', Linear(in_features=100, out_features=100, bias=True)), ('relu8', ReLU()), ('linear9', Linear(in_features=100, out_features=100, bias=True)), ('relu9', ReLU()), ('output', Linear(in_features=100, out_features=10, bias=True))])\n",
            "{'hidden_layers': 10, 'nodes': 100, 'train_accuracy': 0.11155, 'test_accuracy': 0.1028}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=200, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=200, out_features=200, bias=True)), ('relu0', ReLU()), ('output', Linear(in_features=200, out_features=10, bias=True))])\n",
            "{'hidden_layers': 1, 'nodes': 200, 'train_accuracy': 0.9954333333333333, 'test_accuracy': 0.9778}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=200, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=200, out_features=200, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=200, out_features=200, bias=True)), ('relu1', ReLU()), ('output', Linear(in_features=200, out_features=10, bias=True))])\n",
            "{'hidden_layers': 2, 'nodes': 200, 'train_accuracy': 0.99445, 'test_accuracy': 0.9757}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=200, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=200, out_features=200, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=200, out_features=200, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=200, out_features=200, bias=True)), ('relu2', ReLU()), ('output', Linear(in_features=200, out_features=10, bias=True))])\n",
            "{'hidden_layers': 3, 'nodes': 200, 'train_accuracy': 0.9923333333333333, 'test_accuracy': 0.9772}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=200, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=200, out_features=200, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=200, out_features=200, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=200, out_features=200, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=200, out_features=200, bias=True)), ('relu3', ReLU()), ('output', Linear(in_features=200, out_features=10, bias=True))])\n",
            "{'hidden_layers': 4, 'nodes': 200, 'train_accuracy': 0.9923666666666666, 'test_accuracy': 0.97}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=200, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=200, out_features=200, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=200, out_features=200, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=200, out_features=200, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=200, out_features=200, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=200, out_features=200, bias=True)), ('relu4', ReLU()), ('output', Linear(in_features=200, out_features=10, bias=True))])\n",
            "{'hidden_layers': 5, 'nodes': 200, 'train_accuracy': 0.9908666666666667, 'test_accuracy': 0.9693}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=200, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=200, out_features=200, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=200, out_features=200, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=200, out_features=200, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=200, out_features=200, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=200, out_features=200, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=200, out_features=200, bias=True)), ('relu5', ReLU()), ('output', Linear(in_features=200, out_features=10, bias=True))])\n",
            "{'hidden_layers': 6, 'nodes': 200, 'train_accuracy': 0.9882, 'test_accuracy': 0.9758}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=200, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=200, out_features=200, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=200, out_features=200, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=200, out_features=200, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=200, out_features=200, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=200, out_features=200, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=200, out_features=200, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=200, out_features=200, bias=True)), ('relu6', ReLU()), ('output', Linear(in_features=200, out_features=10, bias=True))])\n",
            "{'hidden_layers': 7, 'nodes': 200, 'train_accuracy': 0.9866833333333334, 'test_accuracy': 0.9721}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=200, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=200, out_features=200, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=200, out_features=200, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=200, out_features=200, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=200, out_features=200, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=200, out_features=200, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=200, out_features=200, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=200, out_features=200, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=200, out_features=200, bias=True)), ('relu7', ReLU()), ('output', Linear(in_features=200, out_features=10, bias=True))])\n",
            "{'hidden_layers': 8, 'nodes': 200, 'train_accuracy': 0.09871666666666666, 'test_accuracy': 0.098}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=200, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=200, out_features=200, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=200, out_features=200, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=200, out_features=200, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=200, out_features=200, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=200, out_features=200, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=200, out_features=200, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=200, out_features=200, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=200, out_features=200, bias=True)), ('relu7', ReLU()), ('linear8', Linear(in_features=200, out_features=200, bias=True)), ('relu8', ReLU()), ('output', Linear(in_features=200, out_features=10, bias=True))])\n",
            "{'hidden_layers': 9, 'nodes': 200, 'train_accuracy': 0.11155, 'test_accuracy': 0.1028}\n",
            "OrderedDict([('input', Linear(in_features=784, out_features=200, bias=True)), ('input_activation', ReLU()), ('linear0', Linear(in_features=200, out_features=200, bias=True)), ('relu0', ReLU()), ('linear1', Linear(in_features=200, out_features=200, bias=True)), ('relu1', ReLU()), ('linear2', Linear(in_features=200, out_features=200, bias=True)), ('relu2', ReLU()), ('linear3', Linear(in_features=200, out_features=200, bias=True)), ('relu3', ReLU()), ('linear4', Linear(in_features=200, out_features=200, bias=True)), ('relu4', ReLU()), ('linear5', Linear(in_features=200, out_features=200, bias=True)), ('relu5', ReLU()), ('linear6', Linear(in_features=200, out_features=200, bias=True)), ('relu6', ReLU()), ('linear7', Linear(in_features=200, out_features=200, bias=True)), ('relu7', ReLU()), ('linear8', Linear(in_features=200, out_features=200, bias=True)), ('relu8', ReLU()), ('linear9', Linear(in_features=200, out_features=200, bias=True)), ('relu9', ReLU()), ('output', Linear(in_features=200, out_features=10, bias=True))])\n",
            "{'hidden_layers': 10, 'nodes': 200, 'train_accuracy': 0.11155, 'test_accuracy': 0.1028}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9n9CVbnsuHk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}